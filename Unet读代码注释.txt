1、在Python的numpy库中，经常出现reshape(x，[-1，28，28，1])之类的表达，请问新shape中-1是什么含义？

大意是说，数组新的shape属性应该要与原来的配套，如果等于-1的话，那么Numpy会根据剩下的维度计算出数组的另外一个shape属性值。 


2、CNN训练模型，图片像素要不要除以255

目的是将图片数据转化为实数类型，将0-255的像素值转化为0.0-1.0范围内的实数。大多数图像处理API支持整数和实数类型的输入。但如果输入是整数类型，这些API会在内部将输入转化为实数后处理，再将输出转化为整数。如果有多个处理步骤，在整数和实数之间的反复转化将导致精度损失，因此推荐在图像处理前将其转化为实数类型。

一般我们在用CNN做图像识别时，第一步都是所有数据集除以255，重构图像。


3、keras.layers.ZeroPadding2D什么意思？

作用是在二维矩阵的四周填充0。在卷积操作中，一般使用 padding='SAME' 填充0，但有时不灵活，我们想自己去进行补零操作。
x = keras.layers.ZeroPadding2D(((0, 0), (0, 96-WIDTH)))(inputs)
这一行代码运行完了之后，就从（64，84，1）变成了（64，96，1）。

官方源代码是：__init__(padding=(1, 1), data_format=None, **kwargs)
其中2个整数的2个元组的元组：表示 ((top_pad, bottom_pad), (left_pad, right_pad))，本代码中作者想要表达的含义就是，对原始输入64*88，上下行不增补，左列不增补，右列增补96-84=12个零列。

具体可见： https://www.cnblogs.com/LGJC1314/p/13403811.html


4、Unet的网络结构：

（1）32个5*5卷积核，步数1
（2）LeakyRelu激活层
（3）32个5*5卷积核，步数1
（4）最大池化，pool_size=3, strides=2
（5）LeakyRelu激活层
（6）标准化归一层
------------------------------------
（7）64个5*5卷积核，步数1
（8）LeakyRelu激活层
（9）64个5*5卷积核，步数1
（10）最大池化，pool_size=3, strides=2
（11）LeakyRelu激活层
（12）标准化归一层
-----------------------------------
（13）128个5*5卷积核，步数1
（14）LeakyRelu激活层
（15）128个5*5卷积核，步数1
（16）最大池化，pool_size=3, strides=2
（17）LeakyRelu激活层
（18）标准化归一层
-----------------------------------
（19）128个3*3卷积核，步数1
（20）LeakyRelu激活层
（21）128个3*3卷积核，步数1
（22）最大池化，pool_size=3, strides=2
（23）LeakyRelu激活层
（24）标准化归一层
-----------------------------------
（25）128个3*3卷积核，步数1
（26）LeakyRelu激活层
（27）128个3*3卷积核，步数1
（28）最大池化，pool_size=3, strides=2
（29）LeakyRelu激活层
（30）标准化归一层
-----------------------------------
此时相当于，将feature map分别将尺寸裁减为：  1/2、1/4、1/8、1/16、1/32
U型左半边，卷积结构已经完全结束。后面将是U型右半边，上采样结构。
-----------------------------------
U型中间层，卷积结构，但不再池化了，feature map尺寸没有变化。
（31）128个3*3卷积核，步数1
（32）LeakyRelu激活层
（33）标准化归一层
（34）128个3*3卷积核，步数1
（35）LeakyRelu激活层
（36）标准化归一层
-----------------------------------
U型中间层，反卷积结构，开始融合，还原原来图像尺寸。
-----------------------------------
（37）反卷积层，128个，kernel=5，步数=2
（38）与第24层融合
（39）LeakyRelu激活层
（40）标准化归一层
------------------------------------
（41）反卷积层，128个，kernel=5，步数=2
（42）与第18层融合
（43）LeakyRelu激活层
（44）标准化归一层
------------------------------------
（45）反卷积层，64个，kernel=5，步数=2
（46）与第12层融合
（47）LeakyRelu激活层
（48）标准化归一层
-----------------------------------
（49）反卷积层，64个，kernel=5，步数=2
（50）与第6层融合
（51）LeakyRelu激活层
（52）标准化归一层
----------------------------------
（53）反卷积层，N_CLASSES=11个，kernel=5，步数=2
（54）LeakyRelu激活层
（55）标准化归一层
----------------------------------
（56）反卷积层，N_CLASSES=11个，kernel=5，步数=1
（57）裁减层，出去原先补0的部分，keras.layers.Cropping2D(((0, 0), (0, 96-WIDTH)))(x)
（58）softmax激励层
------------------------------------



5、疑问：BatchNormalization()层到底是让什么东西归一化了？

BatchNormalization层：该层在每个batch上将前一层的激活值重新规范化，即使得其输出数据的均值接近0，其标准差接近1。意思应该是说，使得所有feature map像素的输出值都在0-1之间。

6、为什么要把网络pop掉1个？  layers.pop()到底想干啥？？？

从pop语法上来看，它似乎是把最后append进来的那个元素，又给剔除出去了。感觉作者的意图其实是，最后一个卷积池化的feature map，直接从网络中输出就好了，不需要从前面保留下来的里面去找。

7、作者到底为什么之前要填充0，resize尺寸到64*96呢，之后明明还要crop成64*84？

我的理解是，64*96可以一直用2整除，这样后面上采样还原时就非常方便。而填充的0，其实无纹理特征，它对于整个CNN特征提取融合，其实并无影响。

8、我终于明白，为什么长方体数据要改成“长*宽*p”的形式，而不写成“p*长*宽”的形式？

因为CNN网络conv2D默认输入就是（长，宽，通道数），这里把p和通道数作为等同理解。而且最后前向传播的最终映射输出结果，就是（长，宽，通道数），因此必须把长方体数据要改成“长*宽*p”的形式，才能计算对应的损失函数。


9、啊啊啊啊啊，我终于明白为啥自己重写的网络，比原作者网络训练速度差这么多了...

因为我们用的损失函数不一样，然后优化器也不一样，学习速率也不一样！！！影响居然能有这么大，不过这样才算真实，哪有一训练就有90%准确率的那种.......


10、日了，我终于想明白我cv2.imshow为什么rgb始终只配的出来8种颜色了，默认像素应该是0-1，我全用了0-255，岂不是全部越界了.....

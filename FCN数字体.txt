1、前面使用Data-zoo的数据集，向想都不用想运行结果会出一对问题。目前就遇到很多奇奇怪怪的东西，Data-zoo里的图片尺寸是不相同的，这就导致我定义网络结果的时候很容易出bug。由于物体分为150个类，我直接转化成长方体格式，连内存都不够用。而且150个类分类要求太高了，连单目标都很难跑出结果，更别说这种超级多目标。

我在网上找到了一个很棒的数据集：m2nist，里面有两个npy数据存储文件，直接用np.load读取路径，得到两个（5000，64，84）的数组。第1个数组存储的是灰度图像，里面的像素值从0-255，第2个数组存储的是类别信息，有11类，分别记为0、1、2、...、10，其中0-9代表数字0-9，10代表背景信息忽略不记。


2、在cv显示中，0像素表示的是黑色、1像素表示的是白色。所以显示物体目标时，总算把想要的物体标白。

3、国外写的例子博客真的太强了，我跑通了它的unet，同时也就能学明白它的unet。

https://www.kaggle.com/zhoulingyan0228/m2nist-segmentation-u-net

4、Unet网络与FCN网络有什么区别：https://www.jianshu.com/p/14641b79a672。我发现拿unet跑医疗图像中的结果，真的效果好好，等学完这一节我一定要找一批医疗数据图像，去实验一下强大的效果。

图像分割，简单来说就是给出一张图像，分割出图像的中所需物体的一个完整准确的轮廓，其实也就相当于现实中的“抠图”。这里“抠图”的难度在于，不是由人来抠，如何让机器学会自动帮我们抠，而且要求抠的像素点要很精确，这个是人眼达不到的。

FCN网络主要的亮点在于：
（1）全卷积化。全连接层都变成卷积层，适应任意尺寸输入，输出低分辨率的分割图片。
（2）上采样。上采样可以让图像变成更高分辨率。最简单的方式是重采样和插值：将输入图片进行rescale到一个想要的尺寸，而且计算每个点的像素点。
（3）跳跃结构（Skip Layer）。如果只利用上采样技巧对最后一层的特征图进行上采样，得到原图大小的分割，由于最后一层的特征图太小，我们会损失很多细节。作者提出增加Skips结构将最后一层的预测（有更富的全局信息）和更浅层（有更多的局部细节）的预测结合起来，这样可以在遵守全局预测的同时进行局部预测。

如果padding设置为SAME，则说明输入图片大小和输出图片大小是一致的，如果是VALID则图片经过滤波器后可能会变小。

灰色箭头表示复制和剪切操作，可以发现，在同一层左边的最后一层要比右边的第一层要大一些，这就导致了，想要融合浅层的feature，就要进行一些剪切，也导致了最终的输出是输入的中心某个区域。输出的最后一层，需要使用1x1的卷积层做分类。


5、开始挖掘简化源代码了，又可以上升一大个台阶了！！！
Unet网络在一些文献中也叫做编码器-解码器结构。由于此网络整体结构类似于大写的英文字母U，故得名U-net。U-net与其他常见的分割网络有一点非常不同的地方：U-net采用了完全不同的特征融合方式：拼接，U-net采用将特征在channel维度拼接在一起，形成更厚的特征。而FCN融合时使用的对应点相加，并不形成更厚的特征。U-net建立在FCN的网络架构上，作者修改并扩大了这个网络框架，使其能够使用很少的训练图像就得到很精确的分割结果。

发现一个术语：FC层，fully connected层，也就是全连接层。但是FCN网络是不含FC层的。


